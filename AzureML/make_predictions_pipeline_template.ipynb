{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147285292
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Model, Dataset, Datastore, Experiment, Environment, ScriptRunConfig, RunConfiguration\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.environment import CondaDependencies\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import requests\n",
        "from azure.identity import ChainedTokenCredential,ManagedIdentityCredential\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "import azureml.core\n",
        "from time import sleep\n",
        "import azure.functions as func\n",
        "import json \n",
        "print('SDK version:', azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147333783
        }
      },
      "outputs": [],
      "source": [
        "### connecting to ML workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "### create compute target\n",
        "cpu_cluster_name = \"cpu-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
        "                                                           min_nodes = 0, max_nodes=1)\n",
        "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
        "compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147337330
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# some helper functions to read a file from storage account. These need role permissions to be set up via managed identities.\n",
        "\n",
        "def initialize_storage_account(storage_account_name):\n",
        "    try:\n",
        "        global service_client\n",
        "        MSI_credential = ManagedIdentityCredential()\n",
        "        credential_chain = ChainedTokenCredential(MSI_credential)   \n",
        "        service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\"https\", storage_account_name), credential=credential_chain)    \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def writeFile(container, filepath, filename, data_in):\n",
        "    file_system_client = service_client.get_file_system_client(file_system=container)\n",
        "    directory_client = file_system_client.get_directory_client(filepath)\n",
        "    file_client = directory_client.create_file(filename)\n",
        "    file_client.upload_data(data=data_in,overwrite=True)\n",
        "    return func.HttpResponse(body=\"Successfully saved JSON\",status_code=200)\n",
        "\n",
        "def getFile(container, filepath, filename):\n",
        "    file_system_client = service_client.get_file_system_client(file_system=container)\n",
        "    directory_client = file_system_client.get_directory_client(filepath)\n",
        "    file_client = directory_client.get_file_client(filename)\n",
        "    download = file_client.download_file()\n",
        "    downloaded_bytes = download.readall()\n",
        "    return downloaded_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147340615
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# initalize storage account and get the csv that is to be predicted\n",
        "initialize_storage_account(\"name of your storage account\")\n",
        "payload = getFile(\"container name\", \"folder name\",\"your aml file.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147343136
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# convert the bytes file that is read from storage to a pandas dataframe and remove column that is to be predicted\n",
        "from io import StringIO\n",
        "s=str(payload,'utf-8')\n",
        "data = StringIO(s) \n",
        "df=pd.read_csv(data)\n",
        "# df.drop(df.columns[[0]], axis=1, inplace=True)\n",
        "\n",
        "# modify datatypes so they match the training data set\n",
        "# data1_bool = df.copy()                                     # Create copy of pandas DataFrame\n",
        "# data1_bool['bruises'] = data1_bool['bruises'].map({'t': True, 'f': False})      # Replace string by boolean\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "### create directory for script\n",
        "os.makedirs('./scripts', exist_ok= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686146093866
        }
      },
      "outputs": [],
      "source": [
        "%%writefile scripts/batch_score.py\n",
        "\n",
        "from azureml.core import Workspace, Model, Dataset, Datastore, Run\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication #\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "subscription_id = 'your subscription id'\n",
        "resource_group = 'your resource group name'\n",
        "workspace_name = 'your workspace name'\n",
        "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "### Load model for scoring, overwrite during download if it already exists\n",
        "model = Model(workspace = ws, name='your model name')\n",
        "model.download(exist_ok=True)\n",
        "loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "### Score new data\n",
        "results = loaded_model.predict(data1_bool)\n",
        "df['prediction'] = results\n",
        "\n",
        "# write csv to blob store. this can be improved to write the whole file back and not only predictions + improve formatting\n",
        "writeFile(\"output container name\", \"output folder name\", \"predictions.csv\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147350711
        }
      },
      "outputs": [],
      "source": [
        "### defining run environment\n",
        "myenv = Environment(name=\"myenv\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"joblib\")\n",
        "conda_dep.add_pip_package(\"pandas\")\n",
        "conda_dep.add_pip_package(\"sklearn\")\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")\n",
        "myenv.python.conda_dependencies=conda_dep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147352955
        }
      },
      "outputs": [],
      "source": [
        "### creating run configuration\n",
        "rc = RunConfiguration(script= './scripts/batch_score.py', conda_dependencies = conda_dep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147355310
        }
      },
      "outputs": [],
      "source": [
        "### create pipeline step\n",
        "score_step = PythonScriptStep(name = 'Score step', script_name = 'batch_score.py', source_directory = './scripts', compute_target = compute_target, runconfig = rc, allow_reuse = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147358906
        }
      },
      "outputs": [],
      "source": [
        "### create & validate pipeline\n",
        "steps = [score_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline.validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147364441
        }
      },
      "outputs": [],
      "source": [
        "experiment = Experiment(ws, 'Batch-Scoring-Remote')\n",
        "run = experiment.submit(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686147367743
        }
      },
      "outputs": [],
      "source": [
        "### publish pipeline\n",
        "pipeline.publish(name='your pipeline name')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
